---
layout: post
title: Project 3 - Full Stack Categorical Modeling 
---


### Problem to be Solved
A case study conducted in April of 2016 in Southern and Coastal Brazil concluded that 30% of patients do not show up for their scheduled doctors appointments. Since many patients have health insurance provided by the Brazilian government, this causes a strain in the country's economic resources, and is a general waste of time for doctors. My goal was to create a model that predicts how likely a patient is to arrive for their scheduled appointment.

### Data Aquisition and Back-End Work
I downloaded the original data as a CSV file from Kaggle, and it contained 15 features, and more than 100,000 responses. The features included: PatientID, AppointmentID, Gender, AppointmentOn, ScheduledOn, Age, Neighborhood, Scholarship, Hypertension, Diabetes, Alcoholism, Disability, SMS_received, No-show. I set the response variable, y, equal to the True or False values in the "No-show" column. I will attempt to predict these values as accurately as possible. Next, I trimmed down the features a bit. I transformed as many columns as possible into Boolean values (True/False). I also created a new column called "delta" by subtracting the two dates: AppointmentOn-ScheduledOn. Then, I added another new column called "2+problems" which looked at the "Hypertension, Diabetes, Alcoholism, Disability" columns, and determined if they were seeing the doctor to treat more than one condition. I removed the PatientID, AppointmentID and Neighborhood columns since they were not translatable to the bigger picture.

With a squeaky clean dataset, I created a SQL database in my AWS EC2 instance. Hosting the data in "the cloud" meant I didn't need to store the dataset on my local machine. Instead, I built an engine with SQLalchemy and psychopg2 to query my SQL database and pipe it directly into a Jupyter notebook. 

### Business Logic
Thinking realistically about the model, there was a significant difference between Type I Error and Type II Error. A Type I error would be a false positive, i.e. the doctor expects the patient to arrive, but they do not. This is the way the current medical system works, and we are creating this model to minimize those errors. However, Type II error is a larger problem. A false negative would mean the doctor does not expect the patient to arrive, but they show up. Oh no! This type of error could cause major scheduling backups, and make people question why they even decided to use my model in the first place. So, to minimize Type II error (and avoid tarnishing my reputation), I decided early on that I needed to set a threshold value. After weighing the costs of each type of error, I decided to set the threshold at 0.4. This meant that if the probability of arrival were greater than 0.4, the model would predict the patient to show up. 

### Modeling the Data
Before even beginning to think about modeling, I needed to remedy the imbalanced classes in my dataset. There were nearly four times as many people in the True/showed up category than the False/no-show category. As a quick, solid fix I upsampled the minority class until the Trues/Falses were exacly balanced. From there, I split the data into 70% training set, and 30% testing set. 

#### K-Nearest Neighbors
The theory behind K-Nearest Neighbors is that you can predict the response of any point based on the responses of their neighbors. Nearest neighbors are calculated using Euclidean Distance (think Pythagorean theorem), and the user determines how many neighbors to consider when predicting the new point's response. While optimizing a KNN model, we must determine the value of K that produces the best predictive model. I optimized K with a fine toothed-comb, creating a for-loop that created different models and printed out their F1 Score. (F1 can be described as a metric that combines "Out of all the times I said it was true, how many were right?" and "When it was actually true, how many times did I catch it?")

#### Logistic Regression
Logistic Regression can be thought of as the hipster cousin to Linear Regression. Logistic Regression uses similar coefficients as Linear Regression, but the coefficients correspond to the log odds for the change in each feature. On a graph, Logistic Regression curves start at zero, abruptly bend upward to 1, then plateau again. This creates a very definitive decision boundary.  

#### Naive Bayes
#### Decision Tree
#### Random Forest


![_boroughs](/images/boroughs.png)

