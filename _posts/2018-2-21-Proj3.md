---
layout: post
title: Project 3 - Full Stack Categorical Modeling 
---


### Problem to be Solved
A case study conducted in April of 2016 in Southern and Coastal Brazil concluded that 30% of patients do not show up for their scheduled doctors appointments. Since many patients have health insurance provided by the Brazilian government, this causes a strain in the country's economic resources, and is a general waste of time for doctors. My goal was to create a model that predicts how likely a patient is to arrive for their scheduled appointment.

### Data Aquisition and Back-End Work
I downloaded the original data as a CSV file from Kaggle, and it contained 15 features, and more than 100,000 responses. 

I set the response variable, y, equal to the True or False values in the "No-show" column. I will attempt to predict these values as accurately as possible. Next, I trimmed down the features a bit. I transformed as many columns as possible into Boolean values (True/False). I also created a new column called "delta" by subtracting the two dates: AppointmentOn-ScheduledOn. Then, I added another new column called "2+problems" which looked at the "Hypertension, Diabetes, Alcoholism, Disability" columns, and determined if they were seeing the doctor to treat more than one condition. I removed the PatientID, AppointmentID and Neighborhood columns since they were not translatable to the bigger picture. Once completed, the cleaned features looked like this:

![_Features Chart](/images/featureschart.png)

With a squeaky clean dataset, I created a SQL database in my AWS EC2 instance. Hosting the data in "the cloud" meant I didn't need to store the dataset on my local machine. Instead, I built an engine with SQLalchemy and psychopg2 to query my SQL database and pipe it directly into a Jupyter notebook. 

### Business Logic
Thinking realistically about the model, there was a significant difference between Type I Error and Type II Error. A Type I error would be a false positive, i.e. the doctor expects the patient to arrive, but they do not. This is the way the current medical system works, and we are creating this model to minimize those errors. However, Type II error is a larger problem. A false negative would mean the doctor does not expect the patient to arrive, but they show up. Oh no! This type of error could cause major scheduling backups, and make people question why they even decided to use my model in the first place. So, to minimize Type II error (and avoid tarnishing my reputation), I decided early on that I needed to set a threshold value. After weighing the costs, I decided to set the threshold at 0.4. This meant that if the probability of arrival were greater than 0.4, the model would predict the patient to show up. 

### Pre-modeling
Before even beginning to think about modeling, I needed to remedy the imbalanced classes in my dataset. There were nearly four times as many people in the True/showed up category than the False/no-show category. As a quick, solid fix I upsampled the minority class until the Trues/Falses were exacly balanced. From there, I split the data into 70% training set, and 30% testing set. 

I used 5 different techniques to fit my data: K-Nearest Neighbors, Logistic Regression, Naive Bayes, Decision Trees, and Random Forest. Then, I determined which method was best using success metrics such as F1 Score, Accuracy, and Area Under the ROC Curve. 

#### K-Nearest Neighbors
The theory behind K-Nearest Neighbors is that you can predict the response of any point based on the responses of their neighbors. Nearest neighbors are calculated using Euclidean Distance (think Pythagorean theorem), and the user determines how many neighbors to consider when predicting the new point's response. While optimizing a KNN model, we must determine the value of K that produces the best predictive model. I optimized K with a fine toothed-comb, creating a for-loop that created different models and printed out their F1 Score. (F1 can be described as a metric that combines "Out of all the times I said it was true, how many were right?" and "When it was actually true, how many times did I catch it?") Turns out, the optimal value for k is 30. Yet, even with an optimized k-value, the F1 score and AUC were still not great. F1 Score was 0.66 and the AUC was 0.77. KNN had an accuracy of 70.36%.

![_KNN](/images/KNN.png)

#### Logistic Regression
Logistic Regression can be thought of as the cousin to Linear Regression. Logistic Regression uses similar coefficients as Linear Regression, but the coefficients correspond to the log odds for the change in each feature. On a graph, the Logistic Regression line starts at zero, abruptly bends upward to 1, then plateaus again. This creates a very definitive decision boundary. 

Logistic Regression did not model my data very well because it requires a definitive division between the two classes. Since my data was very mixed and did not have a clear decision boundary, the Logistic Regression model did not know where to place the division. As a result, this model had a terrible score, and predicted that no one ever comes to their doctors appointment. It was a very disappointing model, and was rife with Type II errors. In fact, the results were only slightly better than guessing! The F1 Score was 0.16 (oh dear), and the AUC was 0.623. Logistic Regression had an accuracy of 53.11%.

![_Logistic Regression](/images/LogReg.png)

#### Naive Bayes
A Naive Bayes model generates a probability of True or False by assuming all the features in the dataset are independent. Although we can say with certainty that none of the features are turly independent, this model has strengths in handling binary class data. I gave it a try, but quickly realized that Naive Bayes was only slightly better than guessing, similar to Logistic Regression. The F1 Score was 0.57 and the AUC was 0.672. Naive Bayes had an accuracy of 66.67%.

![_Naive Bayes](/images/NaiveBayes.png)


#### Decision Tree
Decision Trees are nothing more than a series of questions that lead to an answer. Akin to the childhood game "Twenty Questions" the decision tree attempts to correctly predict no-shows in the fewest possible questions.  

![_Decision Tree](/images/tree.png)

After tuning the hyperparameters, I created a decision tree with depth 5, meaning the model was cut off after asking 5 questions. Even though the tree was quite truncated, it performed very well compared to the previous models. The Decision Tree model had an F1 score of 0.60 and the AUC was 0.72. This Decision Tree had an accuracy of 67.56%.

![_Decision Tree](/images/DecisionTree.jpg)


#### Random Forest
Random Forest models are a compilation of many different Decision Trees (hence the name Random "Forest") The randomness comes in because the questions asked at each node are completely random, not using gini or splitting the data by largest groupings. After tuning the hyperparameters, I used a model with 100 different random decision trees, each with a depth of 20. This model had an F1 score of 0.83 and a AUC of 0.90, which made it the best model. In addition, the accuracy was quite high at 81.99%, meaning that it did a good job of predicting correctly.  

![_Random Forest](/images/Random.png)

### Drawing Conclusions from the Data
I superimposed the ROC Curves onto one plot to visualize the differences between my five models. When they are plotted together, it becomes very clear that Random Forest was the best at modeling my patient data. 

![_Flask and D3](/images/flaskapp.png)
