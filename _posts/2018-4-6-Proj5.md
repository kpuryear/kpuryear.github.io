---
layout: post
title: Project 5 - Detecting Exoplanets with NASA
---

## Problem to be Solved:
Each of the stars in the night sky has the potential for multiple planets orbiting around it. These extra-solar planets are called "exoplanets". It is outrageously difficult to detect these planets because of the vast distances between stars, and planets, unlike stars, do not create their own light. In fact, the best way to detect exoplanets is actually through a *lack* of light. As a planet completes its orbit around the host star, there is a non-zero probability that the planet's orbit will cross the disc of the star. If you were able to witness the total solar eclipse in August 2017, you'll know that when some object crosses in front of its star (ex, the Moon), the light from the star (ex, our Sun) becomes significantly dimmer. This detection technique is called the "Transit Method". Using this technique NASA's Kepler Spacecraft has been able to detect 3,708 exoplanets to date. The Kepler mission ran from 2009-2013, and looked at 150,000 main Sequence stars. Now, NASA's TESS (Transiting Exoplanet Survey Satellite) will use the same technique to discover exoplanets at much higher resolution!

## Partnering with NASA
I reached out to NASA when I started working on this project. I knew that the format of the data would probably be in a NASA-specific format, and I needed tools and advice about how to handle it. I emailed the team at Kepler Guest Observer Office, and was surprised when they responded back in mere hours. Dr Christina Hedges and Dr Michael Gully-Santiago were incredibly helpful in getting me set up with access to the Kepler Database, and showed me a package they created called "lightkurve" which specifically deals with transit data from Kepler. Perfect! 

## Data Collection
The raw data that streams directly from Kepler is in the form of .fits images. "Fits" stands for Flux In Time Series. This data is not a typical "image", it has flux data imbedded in it. Flux is a measure of the energy coming from a specific point at a given moment, which is exactly the same as measuring the "brightness" of a star. 

I chose to use data from week 1 of the Kepler Mission. I used Lightkurve to download all the flux images from 9000 stars. Then, I cross checked the data with the NASA Exoplanet Archive, which lists the status of each star, whether there is a confirmed exoplanet or not. As an interesting hiccup in the data collection, the NASAEA only lists data for stars that contain "confirmed" exoplanets or "false positive". None of the stars in my dataset are blank, boring confirmed negatives. This certainly makes my project more challenging and interesting. I combined the information from NASAEA and the raw flux data to obtain my dataset for testing.  

## Data Cleaning and Feature Engineering
I then ran the data through the lightkurve package. I defined a function called TFsinglestarpipe which takes in the KeplerID supplied by NASAEA, and returns a numpy array that contains 1626 values, ie.e. the flux measurement at each hour for that particular star. I pushed the data through a rather convoluted pipeline that looks something like this:
```
def TFsinglestarpipe(kepid):
    try:
        tpf = KeplerTargetPixelFile.from_archive(kepid, quarter=1)  
        lc = tpf.to_lightcurve(aperture_mask=tpf.pipeline_mask)
        flat, trend = lc.flatten(window_length=301, return_trend=True)
        df = flat.to_pandas()
    
        df = df.drop(columns = ['time', 
                        'flux_err', 
                        'quality', 
                        'centroid_row', 
                        'centroid_col',
                        'KeplerID'], axis=1)
    
        df = df.reset_index(drop=True)
        df.index.names = [kepid]
    
        flux = df['flux']
        array = list(flux)
        return array
    except:
        return None
```
I first uploaded the star's Kepler target pixel file from the MAST archive (an in-house, open-source NASA archive) based on the KeplerID. A Target Pixel File is very similar to typical astronomical FITS image, TPFs can be thought of as stacks of images, with one image for every timestamp the telescope took data. I then converted the Target Pixel File to a Light Curve, which creates a graphical representation of the flux over time, plotting flux on the y axis and time on the x axis. From there, I negated any noise trends, converted the flux values into a pandas dataframe, and converted the pandas dataframe to a numpy array.  

I then created my training and test sets manually, creating a list of lists (flux data per star).
```
xtrain = []
ytrain = []

for i in tqdm(range(7000)):
    if TFsinglestarpipe(starlist[i]):
        xtrain.append(TFsinglestarpipe(starlist[i]))
        ytrain.append(dy['Disposition'][i])
    else:
        pass

X_train = np.array(xtrain)
Y_train = np.array(ytrain)
```
In the end, the train set contained records of 5725 stars, and the test set had 2160.
Now on to modeling!

### First Model: Neural Networks using Keras with Tensorflow
Before I started working on this project, Google Brain had just released a blog post about how they designed a neural network that could detect exoplanets from Kepler's data. You can read the blog post [here](https://research.googleblog.com/2018/03/open-sourcing-hunt-for-exoplanets.html). I read this post and was inspired to take on the problem. Thinking, naively, that I knew exactly what I needed to do to create a fully functional exoplanet detection model from scratch. I could not have been more wrong.

Little did I know, the Researchers at Google had actually re-engineered certain Tensorflow packages and commands. By the time they had a working model, their code was barely recognizable to an untrained eye like mine. My inital dream of creating a model from scratch was looking more and more unattainable. 

Never to be deterred, I pressed on and slowly started to chip away at the huge mountain of knowledge needed to create a TensorFlow neural network. Luckily TensorFlow has great documentation, and there are plenty of people who have created online resources. 

### Second Model: Nested Categorical Modeling

### Third Model: Simple Categorical Modeling
