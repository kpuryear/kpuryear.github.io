---
layout: post
title: Project 2 - Linear Regression and Cross Validation
---

For this project, I created a model to predict how many total albums an artist will sell across the globe! For weeks 2 and 3, we focused on predicting numerical data with Linear Regression Models. As a fun choice, I decided to look at the top grossing contemporary musicians of all time. 

### Data Aquisition and Data Cleaning
I aquired my musician data from Wikipedia's article "List of best-selling music artists", last updated in Feb 2018.

[Wikipedia: Best Selling Music Artists](https://en.wikipedia.org/wiki/List_of_best-selling_music_artists)

This page had convenient tables located throughout the article, and it was relatively simple to parse through the HTML to separate out the tables. I used Beautiful Soup to scrape the data directly from HTML into a Pandas dataframe.


![_useful image](/images/wikipedia.png)
![_wiki](/images/wikipedia.png)
PHOTO of first pandas dataframe 
  
  
Once the data was entered into Pandas dataframes, the real cleaning began. There were 5 discrete tables from the webpage, so I concatenated them all together and renumbered the indeces. From there, I devised a regex formula to remove the numbered references that seemed to trail every datapoint. In the webpage they had been actual working links, but to me they were junk. The data was already looking so much cleaner! 


The regex formula I used: (shown here acting on the "Artist" column)

```
killbrackets = re.compile(r' \[[0-9b]+\]')
df["Artist"] = [killbrackets.sub("", i) for i in df["Artist"]]
```


Then, I transformed the "Period Active" column into "Years Active", for example: instead of 1960-1970, it was just 10. For some rows, the data response listed a starting year then said "to present", which I replaced with 2018 and proceeded with my subtraction. 


From there, the real cleaning began. Inside the "Total Certified Units" column was a huge amount of bundled data about the certified sales per country. 

PHOTO of TCU

I devised a large scale attack on this column to break it down into strings, then populate those strings into a dictionary for each band. From there, each small dictionary would be attached to its appropriate band. While this sounds like a simple plan, the execution proved to be much more challenging. I wrote a monsterous regex formula to parse out the individual pieces, then needed 3 nested functions to make this data format happen. At the end of the day, the dictionaries were seamlessly integrated into the large Pandas dataframe, making the dataframe grow from 89x6Â to 89x29.

PHOTO of REGEX and functions

Now with 25 columns at my disposal, I was almost ready to predict Global Sales. First, I iterated through the columns and removed categorical information such as "Genre" and "Artist". Then, I got ready to discard majority of my data, since most of it was rife with NaN values. I deleted all columns that had less than 70 values, and was left with 8 columns (including my target variable, Global Sales).

### Fitting the Linear Regression
To get a better sense of the data, I plotted the correlations with a heatmap and a seaborn pairplot. 

PHOTO of PLOTS

I looked at these plots and found the numerical 'df.corr()' values between each feature, which allowed me to understand which features were the strongest contributors to Global Sales. I used the Statsmodels package to fit a linear regression with all 7 input features to generate a prediction for the Global Sales output result. The P-values corresponding to "FRA" and "UK" were above the acceptable value of 0.05, so I decided to try fitting the model without them. When I fit another linear regression with only 5 features, "Years Active" was found to be colinear with "Start Date". I exluded "Years Active" from my model and was left with 4 features to predict Global Sales: "Start Date", "Claimed Sales", "US" and "AUS". The statsmodels scorecard for this fit is shown below

PHOTO of Statsmodels scorecard

The R-squared value is quite high, at 0.95. Which indicates that 95% of the variability in this data can be explained using this model. The F-statistic is... The P-values for each coefficient are... which indicate...
