---
layout: post
title: Project 2 - Linear Regression and Cross Validation
---

For this project, I created a model to predict how many total albums an artist will sell across the globe! For weeks 2 and 3, we focused on predicting numerical data with Linear Regression Models. As a fun choice, I decided to look at the top grossing contemporary musicians of all time. 

### Data Aquisition and Data Cleaning
I aquired my musician data from Wikipedia's article "List of best-selling music artists", last updated in Feb 2018.

[Wikipedia: Best Selling Music Artists](https://en.wikipedia.org/wiki/List_of_best-selling_music_artists)

This page had convenient tables located throughout the article, and it was relatively simple to parse through the HTML to separate out the tables. I used Beautiful Soup to scrape the data directly from HTML into a Pandas dataframe.


![_wiki](/images/wikipedia.png)
![_first panda](/images/firstpandas.png) 
  
  
Once the data was entered into Pandas dataframes, the real cleaning began. There were 5 discrete tables from the webpage, so I concatenated them all together and renumbered the indeces. From there, I devised a regex formula to remove the numbered references that seemed to trail every datapoint. In the webpage they had been actual working links, but to me they were junk. The data was already looking so much cleaner! 


The regex formula I used: (shown here acting on the "Artist" column)

```
killbrackets = re.compile(r' \[[0-9b]+\]')
df["Artist"] = [killbrackets.sub("", i) for i in df["Artist"]]
```


Then, I transformed the "Period Active" column into "Years Active", for example: instead of 1960-1970, it was just 10. For some rows, the data response listed a starting year then said "to present", which I replaced with 2018 and proceeded with my subtraction. 


From there, the real cleaning began. Inside the "Total Certified Units" column was a huge amount of bundled data about the certified sales per country. 

![_Total Certified Units](/images/tcu.png)

I devised a large scale attack on this column to break it down into strings, then populate those strings into a dictionary for each band. From there, each small dictionary would be attached to its appropriate band. While this sounds like a simple plan, the execution proved to be much more challenging. I wrote a monsterous regex formula to parse out the individual pieces, then needed 3 nested functions to make this data format happen. At the end of the day, the dictionaries were seamlessly integrated into the large Pandas dataframe, making the dataframe grow from 89x6Â to 89x29.

Now with 25 columns at my disposal, I was almost ready to predict Global Sales. First, I iterated through the columns and removed categorical information such as "Genre" and "Artist". Then, I got ready to discard majority of my data, since most of it was rife with NaN values. I deleted all columns that had less than 70 values, and was left with 8 columns (including my target variable, Global Sales).

### Fitting the Linear Regression
To get a better sense of the data, I plotted the correlations with a heatmap and a seaborn pairplot. 

![_heatmap](/images/heatmap.png)
![_sns pairplot](/images/pairplot.png)

I looked at these plots and found the numerical 'df.corr()' values between each feature, which allowed me to understand which features were the strongest contributors to Global Sales. I used the Statsmodels package to fit a linear regression with all 7 input features to generate a prediction for the Global Sales output result. The P-values corresponding to "FRA" and "UK" were above the acceptable value of 0.05, so I decided to try fitting the model without them. When I fit another linear regression with only 5 features, "Years Active" was found to be colinear with "Start Date". I exluded "Years Active" from my model and was left with 4 features to predict Global Sales: "Start Date", "Claimed Sales", "US" and "AUS". The statsmodels scorecard for this fit is shown below

![_score](/images/scorecard.png)

The R-squared value is quite high, at 0.95. Which indicates that 95% of the variability in this data can be explained using this model. The F-statistic is quite high, and the Probability correlating to the F-statistic has a magnitude of 10^-21, meaning that the probability that this linear relationship is due to random chance is virtually 0. The P-values for each coefficient are all very close to 0, which indicate that they are each a crucial part of the model, and the model wouldnot be the same without them. 

### Cross Validation and Regularization
Although my initial model had scored very well on the important statistics, it is always best practice to test a model against out-of-sample data. I created a randomized 70-30 split in my data, and fit a linear regression to the majority. Then I fit that model to the remaining 30% and found the R-squared value. I repeated this pattern 10 times, and took the average of all the R-squared values. The average value ended up being 0.93, which was slightly under the R-squared of my initial model at 0.95. 

As I thought about ways to make my model better, I considered using Elastic Net Regularization. But the purpose of regularization is to push out features that only provide minimal contributions. Since I had very few features, and all of them were very strong correlations, I decided that Regularization was not necessary. 

To illustrate the strength of my model, I have plotted Predicted Sales in blue and Actual Sales in orange. 

![_graph1](/images/model1.png)

Then, I plotted Actual vs. Predicted values. If the model were a perfect fit, all the points would fall exactly along the line y=x. We see some slight deviations, but the model does a fairly accurate job predicting.

![_graph2](/images/model2.png)
