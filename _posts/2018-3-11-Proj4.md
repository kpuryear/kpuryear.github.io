---
layout: post
title: Project 4 - Humor Me, Analyzing Humor
---

## Problem to be Solved
I have always been interested in what makes people laugh. It seems the phrase "sense of humor" means something completely different to each person. I was interested in investigating the construction of jokes, and determining if there are patterns to the type of humor individuals enjoy. To do this, I obtained a dataset that contained 25,000 people's responses to 100 different jokes. Each person rated the "funny-ness" of the joke on a scale from -10 to 10. After analysis, I could finally answer the age old question: "Whats so funny?"

## Business Applications
Aside from being an interesting (and entertaining) topic of investigation, this research has applications for businesses as well. For example, companies with AI bots could use this research to inject (tasteful) humor into their applications such as Siri and Alexa. This would make the AI applications feel more friendly and relatable. Additionally, marketing teams could use this information to create customer humor demographics to direct more targeted ads toward their clients. Humor is a tool people use to create a relaxed and fun atmosphere, and businesses can use that to their advantage.

![_siri](/images/siri.png)

## Analysis Strategy
I decided to use a 2-sided approach to understand this data:
#### Part 1: Analyze the text
- Upload the jokes from html files
- Split up the jokes into individual strings
- Break up the jokes into individual words and create new datasets that illustrates word frequency, using CountVectorizer and TF-IDF.
- Using new datasets, analyze word frequency data using K-Means Clustering and LDA Topic Modeling.
#### Part 2: Analyze the ratings
- Create a collaborative filtering recommender system using the python package sci-kit surprise.  

## Raw Data Looks Like This:
![_rawdata](/images/jokelistraw.png)

## Count Vectorizer and TF-IDF Transformed Datasets
I got started by analyzing the text. I used tools from the NLP toolkit such as count vectorizer and TF-IDF vectorizer to perform sentiment analysis and word similarity. I devided each joke into its own string, then I transformed the strings of text into "ngrams": keeping a table of the number of times certain words appeared in each individual joke. The resulting data table has thousands of features and 100 responses (one per joke). Using the NLP toolkit, I specified ngrams of 2 and 3 meaning the program counted the occurance of 2 and 3 specific words together. 

It is important to organize the jokes this way because machine learning algorithms "learn" by mathematically optimizing numerical data. We need to transform our data from strings into numbers before we can analyze it!

![_count vectorizer](/images/countvectorizer.png)

## K-Means Clustering
K-Means Clustering is a tool for Unsupervised Machine Learning. It assigns randomly-located centroids to the data, and then calculates the distance from the centroid to each nearby point. After it has learned where each point is, the location of the centroid is updated. I used this algorithm to determine how many clusters fit in my data. I checked the algorithm against many different cluster sizes, and used the sillouette score to determine how well each number of clusters fit the data. The number of clusters with the highest sillouette score and lowest SSE (sum of squared error) was k=11.

![_KMeans](/images/KMeanswithline.png)
    

## Latent Dirichlet Allocation Topic Modeling
I used my discoveries from K-Means Clustering to educate my guesses for LDA Topic Modeling. LDA is a unsupervised machine learning technique that groups words into a specified number of "topics". Since my K-Means analysis generated 11 clusters, I used LDA to determine what those 11 topics were. When I ran the topic modeling algorithm, I got jokes split up into discrete topics:
- Married life
- Jokes about women
- Jokes about education
- Jokes about infidelity
- “How many... to screw in a lightbulb”
- “Good News/Bad News”
- Exclamations, nonsense words and words out of context
- Non-US Nationalities
- Family Drama
- Jokes about religion
- Children’s lives/children’s point of view

I generated word clouds to illustrate the words associated with each topic. The size of each word describes its frequency within the topic. 
#### "How Many To Screw in a Lightbulb?"
![_lightbulb](/images/screwinabulb.png)

#### "I've Got Some Good/Bad News"
![_goodnewsbadnews](/images/goodbadnews.png)

#### Family Drama
![_familydrama](/images/familydrama.png)

## Building a Joke Recommender System
I built a recommender system using Scikit-Surprise, a relatively straighforward recommender package. The algorithm uses collaborative filtering technique to predict ratings of users. For my recommender, I created a brand new blank user. Then, as the new user starts rating jokes, their humor profile gets more and more fleshed out, which leads to better recommendations as time goes by. 

## Overall Conclusions
My analysis states that jokes (more specifically, jokes from this dataset) can be classified into 11 distinct categories based on topic. Additionally, my recommender can predict a good joke for any user. These insights can allow marketing teams and AI developers to make their products and advertising campaigns more friendly and relatable in the future. 
